---
title: 'Práctica 2: Limpieza y análisis de datos'
author: "<center> David Dávila y Mónica Gómez </center>"
date: "<center> 2020/01/05 </center>"
output:
  pdf_document:
    toc: yes
    number_sections: yes
    highlight: zenburn
    code_folding: show
    theme: spacelab
    collapse: no
  html_document:
    toc: yes
    number_sections: yes
    df_print: paged
    highlight: zenburn
    code_folding: show
    theme: spacelab
    collapse: no
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r message=FALSE, warning=FALSE}
# getwd()
```

******
# Presentación
******

En esta práctica se elabora un caso práctico orientado a aprender a identificar los datos  relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación  y análisis de las mismas. Para hacer esta práctica tendréis que trabajar en grupos de 2 personas.  Tendréis que entregar un solo archivo con el [enlace Github](https://github.com) donde se  encuentren las soluciones incluyendo los nombres de los componentes del equipo. Podéis utilizar  la Wiki de Github para describir vuestro equipo y los diferentes archivos que corresponden a vuestra entrega. Cada miembro del equipo tendrá que contribuir con su usuario Github. Aunque  no se trata del mismo enunciado, los siguientes ejemplos de ediciones anteriores os pueden  servir como guía: 

● Ejemplo: https://github.com/Bengis/nba-gap-cleaning  
● Ejemplo complejo (archivo adjunto). 

******
# Competencias 
******

En esta práctica se desarrollan las siguientes competencias del Máster de Data Science: 

● Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación  y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.  

● Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración,  transformación, limpieza y validación) para su posterior análisis. 

******
# Objetivos 
******

Los objetivos concretos de esta práctica son:  

- Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de  problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o  multidisciplinares. 
- Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza  y validación) para llevar a cabo un proyecto analítico.  
- Aprender a analizar los datos adecuadamente para abordar la información contenida en  los datos. 
- Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico. 
- Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación. 
- Desarrollar las habilidades de aprendizaje que les permitan continuar estudiando de un  modo que tendrá que ser en gran medida autodirigido o autónomo. 
- Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el  ámbito de la ciencia de datos.

******
# Descripción de la Práctica a realizar 
******

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la  práctica 1 o bien cualquier dataset libre disponible en [Kaggle](https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son: 

● [Red Wine Quality](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)   

● [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) 

El último ejemplo corresponde a una competición activa de Kaggle de manera que,  opcionalmente, podéis aprovechar el trabajo realizado durante la práctica para entrar en esta  competición. Siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes: 

******
# Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?
******

El dataset elegido se llama ‘Credit Card customers’ y puede descargarse desde la plataforma [Kaggle](https://www.kaggle.com/sakshigoyal7/credit-card-customers). Su importancia radica en que trata de resolver un frecuente problema de negocios. 
Este conjunto de datos contiene la información de una agencia bancaria, donde el administrador desea conocer el o los motivos por los cuales están perdiendo clientes, así como prevenir el así llamado ‘Credit Card Churning’. 

Habiendo identificado los clientes que potencialmente desean prescindir de algún servicio bancario, la agencia podría anticipar estrategias para lograr que dichos clientes se queden por más tiempo.

Este dataset contiene información sobre 10,127 clientes. Entre los datos descriptores se encuentran 23 variables que son: 

0   CLIENTNUM:  de tipo entero y sin valores nulos                                                                                                                
1   Attrition_Flag: de tipo entero y sin valores nulos                                                                                                                         
2   Customer_Age: de tipo entero y sin valores nulos                                                                                                                          
3   Gender: de tipo objeto y sin valores nulos                                                                                                                                   
4   Dependent_count: de tipo entero y sin valores nulos                                                                                                                         
5   Education_Level: de tipo objeto y sin valores nulos                                                                                                                        
6   Marital_Status: de tipo objeto y sin valores nulos                                                                                                                        
 7   Income_Category: de tipo objeto y sin valores nulos                                                                                                                       
 8   Card_Category: de tipo objeto y sin valores nulos                                                                                                                            
 9   Months_on_book: de tipo entero y sin valores nulos                                                                                                                           
 10  Total_Relationship_Count: de tipo entero y sin valores nulos                                                                                                                  
 11  Months_Inactive_12_mon: de tipo entero y sin valores nulos                                                                                                                   
 12  Contacts_Count_12_mon: de tipo entero y sin valores nulos                                                                                                                      
 13  Credit_Limit: de tipo flotante y sin valores nulos                                                                                                                           
 14  Total_Revolving_Bal: de tipo entero y sin valores nulos                                                                                                                    
 15  Avg_Open_To_Buy: de tipo flotante y sin valores nulos                                                                                                                        
 16  Total_Amt_Chng_Q4_Q1: de tipo flotante y sin valores nulos                                                                                                                   
 17  Total_Trans_Amt: de tipo entero y sin valores nulos                                                                                                                           
 18  Total_Trans_Ct: de tipo entero y sin valores nulos                                                                                                                            
 19  Total_Ct_Chng_Q4_Q1: de tipo flotante y sin valores nulos                                                                                                                    
 20  Avg_Utilization_Ratio: de tipo flotante y sin valores nulos                                                                                                                
 21  Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1: de tipo objeto y sin valores nulos   
 22  Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2: de tipo flotante y sin valores nulos  

Este dataset se encuentra completamente en el idioma inglés y se trabajará como tal sin hacer traducciones de variables o información, sin embargo se describen en la tabla siguiente aquellas que se tomarán en cuenta para los análisis posteriores:

| Variable | Descripción |
|-|-|
| "CLIENTNUM" | Numero de cliente: Identificador único del cliente titular de la cuenta |
| "Customer_Age" | Variable demográfica: edad del cliente en años |
| "Dependent_count" | Variable demográfica: número de dependientes |
| "Marital_Status" | Variable demográfica: Casado, Soltero, Divorciado, Desconocido |
| "Card_Category" | Variable de producto: tipo de tarjeta (azul, plata, oro, platino) |
| "Total_Relationship_Count" | Número total de productos en poder del cliente |
| "Months_Inactive_12_mon" | Número de meses inactivos en los últimos 12 meses |
| "Contacts_Count_12_mon" | Número de contactos en los últimos 12 meses |
| "Total_Revolving_Bal" | Saldo rotatorio total en la tarjeta de crédito |
| "Total_Amt_Chng_Q4_Q1" | Cambio en el monto de la transacción (Q4 sobre Q1) |
| "Total_Trans_Ct" | Recuento total de transacciones (últimos 12 meses) |
| "Avg_Utilization_Ratio" | Índice de utilización promedio de la tarjeta |
| "Attrition_Flag" | Variable de evento interno (actividad del cliente): si la cuenta está cerrada, entonces 1 más 0 |
| "Gender" | Variable demográfica: M = Hombre, F = Mujer |
| "Education_Level" | Variable demográfica: calificación educativa del titular de la cuenta (ejemplo: escuela secundaria, graduado universitario, etc.) |
| "Income_Category" | Variable demográfica: categoría de ingresos anuales del titular de la cuenta (<$ 40K, $ 40K - 60K, $ 60K - $ 80K, $ 80K- $ 120K,> $ 120K, Desconocido) |
| "Months_on_book" | Periodo de relación con el banco |
| "Credit_Limit" | Límite de crédito en la tarjeta de crédito |
| "Avg_Open_To_Buy" | Línea de crédito abierta para comprar (promedio de los últimos 12 meses) |
| "Total_Trans_Amt" | Monto total de la transacción (últimos 12 meses) |
| "Total_Ct_Chng_Q4_Q1" | Cambio en el recuento de transacciones (Q4 sobre Q1) |
|  |  |
|  |  |


Las modificaciones realizadas (preprocesamiento) se muestran a continuación.

# Integración y selección de los datos de interés a analizar 

## Carga del archivo 

```{r message=FALSE}
#getwd()
```

Se procede a abrir el archivo formato .csv y examinar el tipo de datos con los que R ha interpretado cada variable.  

```{r}
df <- data.frame(read.csv("BankChurners.csv", header=TRUE))
attach(df)
head(df)
```

## Análisis del archivo 

A continuación se muestra un resumen del contenido del dataframe:

```{r}
summary(df)
```

Se revisa la estructura del dataframe:

```{r message=FALSE, warning=FALSE}
str(df) 
```

## Selección de los datos de interés

Para el presente proyecto se utilizarán todas las variables presentes en el juego de datos a excepción de las dos últimas puesto que no se aplicará el análisis Naive Bayes.

```{r}
df <- df[,-(22:23)]
names(df)
```

## Análisis gráfico descriptivo 

Se elaboran gráficas por variable para tener un primer acercamiento y conocer más sobre su comportamiento:  

```{r}
sapply(df,class)
```

```{r message=FALSE, warning=FALSE}
library(ggplot2)

par(mfrow=c(2,2))

#p1 <- plot(CLIENTNUM, main="CLIENTNUM")
p2 <- plot(as.factor(Attrition_Flag), main="Attrition_Flag")
p3 <- hist(Customer_Age, main="Customer_Age")
p4 <- plot(as.factor(Gender), main="Gender")
p5 <- hist(Dependent_count, main="Dependent_count")
p6 <- plot(as.factor(Education_Level), main="Education_Level")
p7 <- plot(as.factor(Marital_Status), main="Marital_Status") 
p8 <- plot(as.factor(Income_Category), main="Income_Category")    
p9 <- plot(as.factor(Card_Category), main="Card_Category")  
p10 <- hist(Months_on_book, main="Months_on_book")
p11 <- plot(as.factor(Total_Relationship_Count), main="Total_Relationship_Count")
p12 <- hist(Months_Inactive_12_mon, main="Months_Inactive_12_mon")  
p13 <- hist(Contacts_Count_12_mon, main="Contacts_Count_12_mon")  
p14 <- hist(Credit_Limit, main="Credit_Limit")  
p15 <- hist(Total_Revolving_Bal, main="Total_Revolving_Bal")  
p16 <- hist(Avg_Open_To_Buy, main="Avg_Open_To_Buy")  
p17 <- hist(Total_Amt_Chng_Q4_Q1, main="Total_Amt_Chng_Q4_Q1")  
p18 <- hist(Total_Trans_Amt, main="Total_Trans_Amt")  
p19 <- hist(Total_Trans_Ct, main="Total_Trans_Ct")  
p20 <- hist(Total_Ct_Chng_Q4_Q1, main="Total_Ct_Chng_Q4_Q1")  
p21 <- hist(Avg_Utilization_Ratio, main="Avg_Utilization_Ratio")  
```

Los gráficos generados nos permiten explorar los datos de forma preliminar. Algunos de los insights que se pueden extraer son:

- Variables cuantitativas: presentan una alta dispersión en sus datos.  

- Variables cuanlitativas: se puede observar que el banco en cuestión presenta una mayor cantidad de clientes con cuentas activas que inactivas, existen más mujeres que hombres, la mayoría están solteros o casados  con estudios terminados y que usan la tarjeta de crédito categoría “Blue”.

******
# Limpieza de los datos
******

El dataset procesado se denomina ‘BankChurners_clean.csv’ y se encuentra alojado en este mismo repositorio.

******
##  Normalización de los datos cuantitativos y cualitativos
******

Estas normalizaciones tienen como objetivo uniformizar los formatos. En este caso no es necesario normalizarlos. Sin embargo, los valores perdidos o valores extremos (de haberlos), se tratarán más adelante.

Se detallan los tipos de datos por variable:  

```{r}
sapply(df, function(x) class(x))
```

******
## ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos? 
******

Este dataset en particular presenta una completitud del 100% como se demuestra a continuación:    

```{r}
sapply(df, function(x) sum(is.na(x)))
```
 
En el caso de haber encontrado elementos vacíos o nulos es conveniente conocer la fuente de los datos antes de proceder a eliminarlos ya que se puede estar perdiendo valiosa información. 
Lo que se hubiera hecho en este caso es una imputación de valores vacíos/nulos y, en medida de lo posible, evitar la eliminación de algún valor.
 
******
## Identificación y tratamiento de valores extremos. 
******

Se procede a hacer la exploración de outliers con el diagráma de caja y bigote, para posteriormente de acuerdo a un análisis por cada variable extraer e imputar los valores con la mediana de aquellas variables donde se haya considerado meritorio hacerlo.  

```{r}

par(mfrow=c(2,2))

#p1 <- boxplot(CLIENTNUM, main="CLIENTNUM")
p3 <- boxplot(Customer_Age, main="Customer_Age")
p5 <- boxplot(Dependent_count, main="Dependent_count")
p10 <- boxplot(Months_on_book, main="Months_on_book")
p12 <- boxplot(Months_Inactive_12_mon, main="Months_Inactive_12_mon")
p13 <- boxplot(Contacts_Count_12_mon, main="Contacts_Count_12_mon")  
p14 <- boxplot(Credit_Limit, main="Credit_Limit")  
p15 <- boxplot(Total_Revolving_Bal, main="Total_Revolving_Bal")  
p16 <- boxplot(Avg_Open_To_Buy, main="Avg_Open_To_Buy")  
p17 <- boxplot(Total_Amt_Chng_Q4_Q1, main="Total_Amt_Chng_Q4_Q1")  
p18 <- boxplot(Total_Trans_Amt, main="Total_Trans_Amt")  
p19 <- boxplot(Total_Trans_Ct, main="Total_Trans_Ct")  
p20 <- boxplot(Total_Ct_Chng_Q4_Q1, main="Total_Ct_Chng_Q4_Q1")  
p21 <- boxplot(Avg_Utilization_Ratio, main="Avg_Utilization_Ratio")  
```


De los diagramas se puede extraer la siguiente información:

- Simetría de la distribución de los datos.
- Detectar la presencia de valores atípicos o outliers.
- Ver cómo es la dispersión de los puntos con la mediana, los percentiles 25 y 75 y los valores máximos y mínimos.

Al revisar los diagramas generados se encuentra que:

La distribución no es simétrica para la mayoría de variables, y es simétrica en los casos de las variables: "Customer_Age", "Dependent_count" y "Months_on_book". 

Para la mayoría de variables se ha optado por mantener los valores originales con excepción de aquellos outliers encontrados para las variables: “”Customer_Age” y Total_Trans_Ct”.

A continuación se detalla un análisis variable por variable:

******
### Customer_Age
******

```{r}
x<-boxplot.stats(Customer_Age)$out
idx <- which(Customer_Age %in% x)
ca <- df$Customer_Age[idx] #Valores atípicos
min(ca)
max(ca)
length(ca)
ca
```
  
En este caso, es un poco extraño que únicamente 2 de 10 mil clientes superen los 70 años y por ello se puede considerar que se trata de valores que probablemente se ingresaron o calcularon erróneamente. Si se tuviera la fecha de nacimiento sería fácil comprobar la edad real de ese par de clientes, sin embargo al desconocer ese dato se procede a imputar.

```{r}
df$Customer_Age[df$Customer_Age>69] <- NA #Dejamos en NA solo los más extremos
df$Customer_Age[idx]
boxplot(df$Customer_Age)
```

```{r}
idx <- which(is.na(df$Customer_Age))
length(idx) #número de valores perdidos

for (i in 1:length(idx)){
index <- idx[i]
df[index,]$Customer_Age <- median(df$Customer_Age, na.rm=TRUE ) #imputación
}
df$Customer_Age[idx] #mostramos el resultado
```

******
### Months_on_book
******

```{r}
x<-boxplot.stats(Months_on_book)$out
idx <- which(Months_on_book %in% x)
mob <- df$Months_on_book[idx] #Valores atípicos
min(mob)
max(mob)
length(mob)
```
  
Como se observa, la variable 'Months_on_book' tiene 386 outliers. Sin embargo, dado que representa el período de pertenencia con la entidad financiera en meses va a ser normal el encontrar clientes muy antiguos (56 meses) o muy recientes (13 meses) por lo que no se los eliminará del dataset.

******
### Months_Inactive_12_mon
******

```{r}
x<-boxplot.stats(Months_Inactive_12_mon)$out
idx <- which(Months_Inactive_12_mon %in% x)
mi <- df$Months_Inactive_12_mon[idx] #Valores atípicos
min(mi)
max(mi)
length(mi)
```
  
La variable 'Months_Inactive_12_mon' tiene 331 outliers. Sin embargo, dado que pueden haber clientes que han permanecido inactivos entre 0-6 meses, este podría ser un parámetro a considerar para saber si el cliente está por suspender o no algún servicio bancario. Por lo tanto, no se los eliminarán del dataset.

******
### Contacts_Count_12_mon
******

```{r}
x<-boxplot.stats(Contacts_Count_12_mon)$out
idx <- which(Contacts_Count_12_mon %in% x)
cc <- df$Contacts_Count_12_mon[idx] #Valores atípicos
min(cc)
max(cc)
length(cc)
```
  
La variable 'Contacts_Count_12_mon' presenta 629 outliers pero dado el contexto de que pueden haber clientes con los cuales no se ha contactado entre 0-6 meses, este podría ser un parámetro a considerar para saber si el cliente está por suspender o no algún servicio bancario. Por lo tanto, no se los eliminarán del dataset.

******
### Credit_Limit
******

```{r}
x<-boxplot.stats(Credit_Limit)$out
idx <- which(Credit_Limit %in% x)
cl <- df$Credit_Limit[idx] #Valores atípicos
min(cl)
max(cl)
length(cl)
```
  
La variable 'Credit_Limit' presenta 984 outliers pero dado que el límite de crédito puede variar dependiendo de muchos factores para a un cliente en específico se opta por mantenerlos.

******
### Avg_Open_To_Buy
******

```{r}
x<-boxplot.stats(Avg_Open_To_Buy)$out
idx <- which(Avg_Open_To_Buy %in% x)
avg <- df$Avg_Open_To_Buy[idx] #Valores atípicos
min(avg)
max(avg)
length(avg)
```
  
Dado que la variable 'Avg_Open_To_Buy' presenta 963 outliers y representa línea de crédito abierta para compra se mantienen los valores.

******
### Total_Amt_Chng_Q4_Q1
******

```{r}
x<-boxplot.stats(Total_Amt_Chng_Q4_Q1)$out
idx <- which(Total_Amt_Chng_Q4_Q1 %in% x)
tac <- df$Total_Amt_Chng_Q4_Q1[idx] #Valores atípicos
min(tac)
max(tac)
length(tac)
```
  
La variable 'Total_Amt_Chng_Q4_Q1' tiene 1954 outliers y muestra los cambios transaccionales que pueden ser muy variables dependiendo del perfil de cada cliente, así que se opta por mantenerlos.

******
### Total_Trans_Amt
******

```{r}
x<-boxplot.stats(Total_Trans_Amt)$out
idx <- which(Total_Trans_Amt %in% x)
tta <- df$Total_Trans_Amt[idx] #Valores atípicos
min(tta)
max(tta)
length(tta)
```
  
La variable 'Total_Trans_Amt' tiene 896 outliers y muestra la totalidad de cantidad de transacciones que pueden ser muy variables de persona a persona, así que se opta por mantenerlos.

******
### Total_Trans_Ct
******

```{r}
x<-boxplot.stats(Total_Trans_Ct)$out
idx <- which(Total_Trans_Ct %in% x)
ttc <- df$Total_Trans_Ct[idx] #Valores atípicos
min(ttc)
max(ttc)
length(ttc)
```
  
La variable 'Total_Trans_Ct' tiene 2 outliers. Es extraño que existan únicamente dos clientes que durante los últimos 12 meses hayan realizado casi 140 transacciones. Por ello se opta por imputar esos datos.


```{r}
df$Total_Trans_Ct[df$Total_Trans_Ct>137] <- NA #Dejamos en NA solo los más extremos
df$Total_Trans_Ct[idx]
boxplot(df$Total_Trans_Ct)
```

```{r}
idx <- which(is.na(df$Total_Trans_Ct))
length(idx) #número de valores perdidos

for (i in 1:length(idx)){
index <- idx[i]
df[index,]$Total_Trans_Ct <- median(df$Total_Trans_Ct, na.rm=TRUE ) #imputación
}
df$Total_Trans_Ct[idx] #mostramos el resultado
```

# Exportación de los datos preprocesados

```{r}
# Exportación de los datos limpios en formato .csv
write.csv(df, "BankChurners_clean.csv")
```


# Análisis de los datos. 

## Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar). 

```{r message=FALSE}
library(MASS)
library(dplyr)
library(plyr)
```

Para este estudio se tomarán algunas variables de tipo categórico para observar si se comportan de una forma distinta ante la variable que deseeamos predecir. Antes de ello se eliminará la variable CLIENTUM de nuestro conjunto de datos.

```{r message=FALSE}
data=subset(df, select=-c(CLIENTNUM))
colnames(data)
```

En este caso nos interesarán los grupos correspondientes al género, el nivel de educación y estado marital.

```{r message=FALSE}
#Selección de grupos.
#Agrupación por genero.
data.femenino<-data[data$Gender=="F",]
data.masculino<-data[data$Gender=="M",]
#Agrupación por nivel de educación
data.HighSchool<-data[data$Education_Level=="High School",]
data.Graduate<-data[data$Education_Level=="Graduate",]
data.Uneducated<-data[data$Education_Level=="Uneducated",]
data.Unknown<-data[data$Education_Level=="Unknown",]
data.College<-data[data$Education_Level=="College",]
data.Postgraduate<-data[data$Education_Level=="Post-Graduate",]
data.Doctorate<-data[data$Education_Level=="Doctorate",]
#Agrupación por estado marital
data.Married<-data[data$Marital_Status=="Married",]
data.Single<-data[data$Marital_Status=="Single",]
data.UnknownStatus<-data[data$Marital_Status=="Unknown",]
data.Divorced<-data[data$Marital_Status=="Divorced",]
```

Por último dado que nuestra variable a predecir se encuentra con los valores "Existing Customer" y "Attrited Customer" se procede a cambiarla con notación binaria. Es decir, "Attrited Customer" tomará el valor 1 y "Existing Customer" tomará el valor 0.

```{r message=FALSE}
#Se convierten a dummy
data$Attrition_Flag<-ifelse(data$Attrition_Flag=="Attrited Customer",1,0)
```

## Comprobación de la normalidad y homogeneidad de la varianza. 
Es de principal importancia dependiendo del modelo a aplicar, el realizar la correspondiente comprobación de normalidad y homogeneidad de la varianza en nuestro conjunto de datos. Por lo cual, para realizar la comprobación de nuestras variables sobre una población normalmente distribuida, se utilizará la prueba de normalidad de Anderson-Darling.

Teniendo en cuenta un $\alpha =0.05$, si obtenemos un p-valor superior al alpha mencionado anteriormente, entonces no rechazaremos nuestra hipótesis nula y los valores vendrán de una distribución/población normal.

```{r message=FALSE}
library(nortest)
col.names=colnames(data)
alpha = 0.05

for (i in 1:ncol(data)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n")
  if (is.integer(data[[i]]) | is.numeric(data[[i]])) {
    p_val = ad.test(data[[i]])$p.value
    if (p_val < alpha) {
      cat(col.names[i])
      # Format output
      if (i < ncol(data) - 1)
      if (i %% 1 == 0) cat("\n")
    }
  }
}
```

Posterior a ello, realizamos la comprobación sobre la homogeneidad de las varianzas con el test de Fligner.Killen. Para nuestro caso, estudiaremos la homogeneidad en cuanto a los grupos conformados por las siguientes variables: Género, nivel educativo, estado marital, categoría de ingresos y categoría de la tarjeta. Recordemos que la hipótesis nula consiste en que las varianzas de los grupos son iguales.

```{r message=FALSE}
fligner.test(Attrition_Flag~Gender, data=data)
fligner.test(Attrition_Flag~Education_Level, data=data)
fligner.test(Attrition_Flag~Marital_Status, data=data)
fligner.test(Attrition_Flag~Income_Category, data=data)
fligner.test(Attrition_Flag~Card_Category, data=data)
```

Teniendo en cuenta los resultados anteriores, solamente podemos no rechazar la hipótesis nula para las variables: nivel educativo, estado marital y categoría de la tarjeta. Dado que se obtienen que el valor-p asociado a la prueba es mayor que el alpha establecido.

## Aplicación de pruebas estadísticas para comparar los grupos de datos. En función  de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis,  correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis  diferentes. 

### Correlaciones
```{r message=FALSE}
corr_matrix <- matrix(nc = 2, nr = 0)
colnames(corr_matrix) <- c("estimate", "p-value")
# Calcular el coeficiente de correlación para cada variable cuantitativa
# con respecto al campo "precio"
for (i in 1:(ncol(data) - 1)) {
  if (is.integer(data[[i]]) | is.numeric(data[[i]])) {
    spearman_test = cor.test(data[[i]],
                             data[[1]],
                             method = "spearman")
    corr_coef = spearman_test$estimate
    p_val = spearman_test$p.value
    # Add row to matrix
    pair = matrix(ncol = 2, nrow = 1)
    pair[1][1] = corr_coef
    pair[2][1] = p_val
    corr_matrix <- rbind(corr_matrix, pair)
    rownames(corr_matrix)[nrow(corr_matrix)] <- colnames(data)[i]
  }
}

corr_matrix
```
Los valores que estamos observando anteriormente de correlación, no conllevan una fuerte correlación teniendo en cuenta nuestra variable objetivo, ya que por lo general están variando entre $-0.37$ y $0.18$, por lo cual se pueden clasificar dichos valores como una correlación débil.


### Contraste de hipótesis
Para ello se utilizará la prueba de Fisher, la cual es un test exacto cuando se quiere observar si existe asociación entre dos variables de tipo cualitativo, es decir, si las proporciones de una variable son diferentes dependiendo del valor que adquiera la otra variable.

**H0:** Las variables son independientes por lo que una variable no varia entre los distintos niveles de la otra variable.

**H1:** Las variables son dependientes, una variable varia entre los distintos niveles de la otra variable.

```{r message=FALSE}
tabla <- table(data$Gender, data$Attrition_Flag)
tabla
```
**Test de Fisher.**

```{r message=FALSE}
fisher.test(x = tabla, alternative = "two.sided")

```

Teniendo en cuenta el valor p observado, es igual a $0.0001825$ es menor que el nivel de significancia 0.05, rechazamos la hipótesis nula. Es decir, las variables de Attrition_Flag y Gender son dependientes. Este pequeño análisis nos ayudará a futuro para ingresar las variables en el modelo de regresión logística.

### Modelo de regresión logística
Se plantea un primer modelo de regresión logística teniendo en cuenta que nuestra variable objetivo es binaria. Además se traen variables de interés en las cuales trataremos de predecir la estancia de los clientes dentro de la empresa. Entre dichas variables encontramos por ejemplo: edad del usuario, género, si tiene personas que dependen del usuario, nivel educativo, estado marital, categoría de ingresos, categoría de la tarjeta entre otros.

```{r message=FALSE}
logisticModelFull <- glm(Attrition_Flag ~  Customer_Age+Gender+Dependent_count+
                           Education_Level+Marital_Status+Income_Category+
                           Card_Category+Months_on_book+
                           Total_Relationship_Count+Months_Inactive_12_mon
                         +Contacts_Count_12_mon+Total_Revolving_Bal+
                           Total_Trans_Amt+Total_Trans_Ct,
                         family = "binomial",data)
summary(logisticModelFull)
```

Dentro del resumen de los resultados del modelo obtenemos que las siguientes variables son estadísticamente signficativas: Gender, Dependent_count, Educational_Level, Marital_Status, Income_Category,Card_Category, Total_Relationship_Count, Months_Inactive_12_mon, Contacts_Count_12_mon, Total_Revolving_Bal, Total_Trans_Amt y Total_Trans_Ct.

Sin embargo, utilizaremos la función stepAIC de R, la cual realiza la selección del modelo paso a paso por el criterio de información de Akaike (AIC), el cual es una medida de calidad relativa de un modelo estadístico.

```{r message=FALSE}
logisticModelNew <-  stepAIC(logisticModelFull, trace = 0)
summary(logisticModelNew)
```
Finalmente mediante esta función se elige el modelo con las siguientes variables: Customer_Age, Gender, Dependent_count, Marital_Status, Income_Category, Card_Category, Total_Relationship_Count, Months_Inactive_12_mon, Contacts_Count_12_mon, Total_Revolving_Bal, Total_Trans_Amt, Total_Trans_Ct.

### Validación cruzada

Por otra parte realizaremos un acercamiento con validación cruzada con el modelo anteriormente visto, y observar si se realizan algunos cambios dentro del modelo, en particular se utilizará el método Validación cruzada de K-fold. Recordando que este método evalua el rendimiento del modelo en diferentes subconjuntos de los datos de entrenamiento y luego calcula la tasa de error de predicción promedio. En la práctica, normalmente se realiza una validación cruzada de k veces utilizando $k=5$ o $k=10$, por lo cual utilizaremos $k=10$.


Primero, se decide realizar una división de los datos en los conjuntos de datos de entrenamiento y de prueba. En este caso se genera una variable aleatoria que toma valores 1 y 0 gracias a que proviene de una distribución binomial con una probabilidad de 0.66.
```{r message=FALSE}
library(caret)
set.seed(27345)

data$isTrain <- rbinom(nrow(data),1,0.66)
train <-  data %>% filter(data$isTrain =="1")
test <- data %>%  filter(data$isTrain == "0")
train_control <- trainControl(method = "cv", number = 10)
model <- train(Attrition_Flag ~ Customer_Age + Gender + Dependent_count + 
                 Marital_Status + Income_Category + Card_Category + Total_Relationship_Count + 
                 Months_Inactive_12_mon + Contacts_Count_12_mon + Total_Revolving_Bal + 
                 Total_Trans_Amt + Total_Trans_Ct,
               data = train,
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(model)
```
Sin embargo, observamos que en este acercamiento se encuentra que Card_Category no es significativo estadisticamente en ninguno de sus niveles por lo cual volvemos a plantear el modelo sin dicha categoría.
```{r message=FALSE}
library(caret)
set.seed(27345)

data$isTrain <- rbinom(nrow(data),1,0.66)
train <-  data %>% filter(data$isTrain =="1")
test <- data %>%  filter(data$isTrain == "0")
train_control <- trainControl(method = "cv", number = 10)
model <- train(Attrition_Flag ~ Customer_Age + Gender + Dependent_count + 
                 Marital_Status + Income_Category + Total_Relationship_Count + 
                 Months_Inactive_12_mon + Contacts_Count_12_mon + Total_Revolving_Bal + 
                 Total_Trans_Amt + Total_Trans_Ct,
               data = train,
               trControl = train_control,
               method = "glm",
               family=binomial())
summary(model)
```
Algo que podemos observar teniendo en cuenta la validación cruzada junto con el modelo anteriormente propuesto es el valor del Criterio de Akaike que toma. Por ejemplo para el modelo *LogisticModelNew* tenemos un valor de $AIC=5155$ mientras que para el modelo con validación cruzada se tiene un valor de $AIC=3456.5$. Esto podría llegar a ser un indicador, para tener en cuenta una métodología (K-Fold) sobre otra.

# Representación de los resultados a partir de tablas y gráficas. 

## Modelo de regresión lineal

### Interpretacion de coeficientes (odds)

```{r message=FALSE}
odds <-  coef(logisticModelNew) %>% exp() %>% round(2)
odds
```
Una breve interpretación de los resultados obtenidos con los coeficientes u "odds", puede ser la siguiente:

* Los usuarios que se encuentran solteros aumentan la tasa de abandono en un 11%.
* Dentro de los usuarios que tienen diferentes categoráis de tarjetas se tiene que aquellos que tienen la categoría platino aumentan la tasa de abandono en un 119%.
* Una relación inversa que observamos la tasa de abandono es un un 59% menor si el usario es de género masculino comparado con las usuarias femeninas.

### Predicciones

Se realiza el modelo escogido anteriomente con el conjunto de datos de entrenamiento, y se realizan las predicciones con la función predict con el conjunto de datos de prueba.

```{r message=FALSE}
LogisticTrainNew <- glm(formula = Attrition_Flag ~ Customer_Age + Gender + Dependent_count + 
                          Marital_Status + Income_Category + Card_Category + Total_Relationship_Count + 
                          Months_Inactive_12_mon + Contacts_Count_12_mon + Total_Revolving_Bal + 
                          Total_Trans_Amt + Total_Trans_Ct, family = "binomial", data = train)

#prediciton 
test$predictNew <- predict(LogisticTrainNew , type = "response" , newdata = test)
```

Para revisar que tan bien el modelo logístico predice los valores del conjunto de prueba se procede a realizar la matriz de confusión.

```{r message=FALSE}
library(readr)
library(ggplot2)
library(boot)
library(e1071)
predicciones=as.factor(ifelse(test = test$predictNew > 0.35, yes = 1, no = 0))
observaciones=as.factor(test$Attrition_Flag)
matriz<-confusionMatrix(observaciones, predicciones)
matriz
```

Como podemos observar, el modelo de reresión logística planteado nos da un nivel de exactitud de 88.87%, lo cual nos puede indicar la falta de variables exogenas que nos ayuden a modelar nuestro datos actuales.

```{r message=FALSE}
library(pROC)
test_prob = predict(logisticModelNew, newdata = test, type = "response")
test_roc = roc(test$Attrition_Flag ~ test_prob, plot = TRUE, print.auc = TRUE)
```

Por otra parte se realiza la grafíca ROC en la cual contiene también el valor de AUC (más conocida como el area bajo la curva ROC), hay que recordar que este valor varia entre 0 y 1, donde un modelo cuyas predicciones son un 100% incorrectas tiene un AUC de 0.0, otras donde sus predicciones son de un 100% entonces su valor de AUC asociado es de 1.0. En nuestro caso, obtuvimos un valor de AUC igual a 0.922 lo cual nos indica que las predicciones que se están haciendo se desvian un poco pero están cerca de pertenecer a un modelo "perfecto".

## Modelo de regresión lineal con validación cruzada

Para el modelo de regresión logística con validación cruzada se realizan las correspondientes predicciones para el conjunto de datos de test y generamos la curva ROC.

```{r message=FALSE}
library(ggplot2)
library(ROCR)

predict0 <- predict(model, type = 'raw',test)
ROCRpred0 <- prediction(as.numeric(predict0),as.numeric(test$Attrition_Flag))
ROCRperf0<- performance(ROCRpred0, 'tpr', 'fpr')
plot(ROCRperf0, colorize=TRUE, text.adj=c(-0.2,1.7))
```

Por otra parte, comparando las predicciones generadas por el método de validación cruzada no varian comparado con el modelo anteriormente presenteado, al igual que los valores de precisión, sensibilidad y especificidad.

```{r message=FALSE}
predicciones2=as.factor(ifelse(test = predict0 > 0.35, yes = 1, no = 0))
observaciones2=as.factor(test$Attrition_Flag)
matriz2<-confusionMatrix(observaciones2, predicciones2)
matriz2
```

# Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las  conclusiones? ¿Los resultados permiten responder al problema?

Teniendo en cuenta los resultados por ejemplo de la exactitud y la precisión presentada dentro del código se recomendaría el poder utilizar nuevas variables que nos ayuden a encontrar relaciones escondidas dentro de los datos y generen un mejor modelo con una exactitud y precisión más alta. Sin embargo los resultados actuales que se tienen del modelo de regresíón logístico son bastante buenos teniendo en cuenta los datos y la cantidad de datos que se obtuvieron para la realización de la práctica. Otra opción que tendíamos a futuro, sería plantear otros tipos de modelos que se comparen con el modelo actual, tal como lo puede ser un árbol de decisión, clústeres e inclusive redes neuronales para manejar las relaciones complejas que no se pueden detectar utilizando otros modelos anteriormente mencionados.  

Por otra parte, respondiendo a nuestra pregunta planteada y a partir de los resultados obtenidos mediante el modelo de regresión logistica podemos modelar y predecir a cierto nivel la deserción de los clientes dentro de la empresa.

# Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la  limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en  Python.  

Project Link:https://github.com/wickedlexie/AnalisisPRA2

# Recursos   

Los siguientes recursos son de utilidad para la realización de la práctica:   

● Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos.  Editorial UOC.   
● Megan Squire (2015). Clean Data. Packt Publishing Ltd.   
● Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques.  Morgan Kaufmann.   
● Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme  Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.   
● Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.  
● Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.   
● Tutorial de Github: https://guides.github.com/activities/hello-world. 

# Criterios de valoración   

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente: 

Los apartados 1, 2 y 6 valen 0,5 puntos.   
Los apartados 3, 5 y 7 valen 2 puntos.   
El apartado 4 vale 2,5 puntos.   

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las diferentes  etapas deberán justificarse y acompañarse del código correspondiente. También se valorará la  síntesis y claridad, a través del uso de comentarios, del código resultante, así como la calidad de  los datos finales analizados. 

# Formato y fecha de entrega  

Durante la semana del 21 al 25 de diciembre el grupo podrá entregar al profesor una entrega  parcial opcional. Esta entrega parcial es muy recomendable para recibir asesoramiento sobre la  práctica y verificar que la dirección tomada es la correcta. Se entregarán comentarios a los  estudiantes que hayan efectuado la entrega parcial pero no contará para la nota de la práctica.  

En la entrega parcial los estudiantes deberán entregar por correo electrónico, al profesor  encargado del aula, el enlace al repositorio Github con el que hayan avanzado.   

En referente a la entrega final, hay que entregar un único fichero que contenga el enlace Github,  el cual no se podrá modificar posteriormente a la fecha de entrega, donde haya:  

1. Una Wiki con los nombres de los componentes del grupo y una descripción de los  ficheros.
2. Un documento PDF con las respuestas a las preguntas y los nombres de los componentes  del grupo. Además, al final del documento, deberá aparecer la siguiente tabla de  contribuciones al trabajo, la cual debe firmar cada integrante del grupo con sus iniciales. Las iniciales representan la confirmación de que el integrante ha participado en dicho  apartado. Todos los integrantes deben participar en cada apartado, por lo que,  idealmente, los apartados deberían estar firmados por todos los integrantes. 

| **Contribuciones** | **Firma** |
|-:|:-:|
| Investigación previa  | DFDO (dfdavila)<br>Mónica Alexandra Gómez Martínez |
| Redacción de las respuestas  | DFDO (dfdavila)<br>Mónica Alexandra Gómez Martínez |
| Desarrollo código  | DFDO (dfdavila)<br>Mónica Alexandra Gómez Martínez |

3. Una carpeta con el código generado para analizar los datos. 
4. El fichero CSV con los datos originales. 
5. El fichero CSV con los datos finales analizados.  

Este documento de entrega final de la Práctica 2 se debe entregar en el espacio de Entrega y  Registro de AC del aula antes de las 23:59 del día 5 de enero. No se aceptarán entregas fuera  de plazo.